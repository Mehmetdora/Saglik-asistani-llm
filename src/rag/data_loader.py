"""
CSV verisini yÃ¼kle ve RAG iÃ§in hazÄ±rla
"""

import pandas as pd
import json
from pathlib import Path
from typing import List, Dict


class DiseaseDataLoader:

    def __init__(self, csv_path: str):
        self.csv_path = Path(csv_path)
        self.df = None
        self.processed_data = []

    def load_csv(self):
        """CSV'yi oku"""
        print(f"ğŸ“‚ CSV yÃ¼kleniyor: {self.csv_path}")

        try:
            self.df = pd.read_csv(self.csv_path, encoding="utf-8")
            print(f"âœ… {len(self.df)} satÄ±r yÃ¼klendi")
            return True
        except Exception as e:
            print(f"âŒ Hata: {e}")
            return False

    def process_diseases(self):
        """Her hastalÄ±ÄŸÄ± RAG iÃ§in uygun formata Ã§evir"""

        print("ğŸ”„ Veriler iÅŸleniyor...")

        for idx, row in self.df.iterrows():
            # SÃ¼tun isimlerini kendi CSV'ne gÃ¶re deÄŸiÅŸtir!
            # Ã–rnek varsayÄ±lan isimler:
            print(row.get("bolum"))
            disease_doc = self._create_disease_document(
                hastalÄ±k=row.get("hastalÄ±k", row.get("HastalÄ±k", "")),
                nedir=row.get("nedir", row.get("Nedir", "")),
                belirtiler=row.get("belirtiler", row.get("Belirtiler", "")),
                teshis=row.get("teshis", row.get("TeÅŸhis", "")),
                tedavi=row.get("tedavi", row.get("Tedavi", "")),
                bolum=str(row.get("bolum", "")).strip(),
                turler=row.get("tÃ¼rleri", row.get("TÃ¼rler", "")),
                soru_cevap=row.get("soru_cevap", row.get("Soru-Cevap", "")),
                link=row.get("link", row.get("Link", "")),
            )

            self.processed_data.append(disease_doc)

        print(f"âœ… {len(self.processed_data)} hastalÄ±k iÅŸlendi")
        return self.processed_data

    def _create_disease_document(self, **kwargs):
        """
        Her hastalÄ±k iÃ§in RAG'a uygun dÃ¶kÃ¼man oluÅŸtur

        Ã–NEMLÄ°: Her hastalÄ±k birden fazla "chunk"a bÃ¶lÃ¼nebilir
        """

        hastalik_adi = kwargs.get("hastalÄ±k", "").strip()

        # Ana dÃ¶kÃ¼man metni - RAG'Ä±n arayacaÄŸÄ± text
        # Bu metni dÃ¼zenlerken kullanÄ±cÄ±nÄ±n soracaÄŸÄ± sorularÄ± dÃ¼ÅŸÃ¼n

        document_text = f"""
            HASTALIK: {hastalik_adi}

            NE: {kwargs.get('nedir', '')}

            BELÄ°RTÄ°LER: {kwargs.get('belirtiler', '')}

            TÃœRLER: {kwargs.get('tÃ¼rleri', '')}

            TEÅHÄ°S: {kwargs.get('teshis', '')}

            TEDAVÄ°: {kwargs.get('tedavi', '')}

            SORU-CEVAP: {kwargs.get('soru_cevap', '')}
        
        """.strip()

        # Metadata - Filtreleme iÃ§in kullanÄ±lacak
        metadata = {
            "hastalÄ±k": hastalik_adi,
            "bolum": kwargs.get("bolum", "").strip(),
            "link": kwargs.get("link", "").strip(),
            "doc_type": "disease",
        }

        return {
            "id": hastalik_adi.lower().replace(" ", "_"),
            "text": document_text,
            "metadata": metadata,
        }

    def chunk_large_documents(self, max_chunk_size=1000):
        """
        Uzun dÃ¶kÃ¼manlarÄ± parÃ§ala

        RAG iÃ§in optimal chunk boyutu: 500-1500 karakter
        """
        print("âœ‚ï¸  Uzun dÃ¶kÃ¼manlar parÃ§alanÄ±yor...")

        chunked_data = []

        for doc in self.processed_data:
            text = doc["text"]

            # EÄŸer metin Ã§ok uzunsa, paragraflara bÃ¶l
            if len(text) > max_chunk_size:
                paragraphs = text.split("\n\n")

                for i, para in enumerate(paragraphs):
                    if para.strip():
                        chunk_doc = {
                            "id": f"{doc['id']}_chunk_{i}",
                            "text": para.strip(),
                            "metadata": {
                                **doc["metadata"],
                                "chunk_index": i,
                                "is_chunk": True,
                            },
                        }
                        chunked_data.append(chunk_doc)
            else:
                chunked_data.append(doc)

        print(f"âœ… {len(self.processed_data)} dÃ¶kÃ¼man â†’ {len(chunked_data)} chunk")
        self.processed_data = chunked_data
        return chunked_data

    def save_processed_data(self, output_path):
        """Ä°ÅŸlenmiÅŸ veriyi JSON olarak kaydet"""

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.processed_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Ä°ÅŸlenmiÅŸ veri kaydedildi: {output_file}")

    def get_statistics(self):
        """Veri istatistiklerini gÃ¶ster"""

        if not self.processed_data:
            print("âš ï¸  HenÃ¼z veri iÅŸlenmedi")
            return

        bolumler = {}
        for doc in self.processed_data:
            bolum = doc["metadata"].get("bolum", "Bilinmiyor")
            bolumler[bolum] = bolumler.get(bolum, 0) + 1

        print("\nğŸ“Š Veri Ä°statistikleri:")
        print(f"   - Toplam dÃ¶kÃ¼man: {len(self.processed_data)}")
        print(f"   - BÃ¶lÃ¼m sayÄ±sÄ±: {len(bolumler)}")
        print("\n   En Ã§ok hastalÄ±k olan bÃ¶lÃ¼mler:")

        sorted_bolumler = sorted(bolumler.items(), key=lambda x: x[1], reverse=True)
        for bolum, count in sorted_bolumler[:10]:
            print(f"     â€¢ {bolum}: {count} hastalÄ±k")


# Test iÃ§in main
if __name__ == "__main__":
    loader = DiseaseDataLoader("data/raw/hastaliklar_detayli_listesi.csv")

    if loader.load_csv():
        loader.process_diseases()
        loader.chunk_large_documents(max_chunk_size=1000)
        loader.save_processed_data("data/processed/diseases_processed.json")
        loader.get_statistics()
