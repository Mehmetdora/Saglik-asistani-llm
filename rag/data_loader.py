import pandas as pd
import json
from pathlib import Path
from typing import List, Dict


class DiseaseDataLoader:

    def __init__(self, csv_path: str):
        self.csv_path = Path(csv_path)
        self.df = None
        self.processed_data = []
        
        
        """
        
        HazÄ±rlanan CSV dosyasÄ± alÄ±nÄ±r , RAG sisteminin kullanabileceÄŸi ÅŸekilde
        dÃ¶kÃ¼man + metadata + chunk olarak parÃ§alar. 
        SonrasÄ±nda bu her bir chunk parÃ§asÄ±nÄ± bir json dosyasÄ±na kaydeder. 
        
        """

    def load_csv(self):
        """
        csv yi oku , 
        self.df e kaydet. 
        """
        print(f"ğŸ“‚ CSV yÃ¼kleniyor: {self.csv_path}")

        try:
            self.df = pd.read_csv(self.csv_path, encoding="utf-8")
            print(f"âœ… {len(self.df)} satÄ±r yÃ¼klendi")
            return True
        except Exception as e:
            print(f"âŒ Hata: {e}")
            return False
    
    """
    self.df iÃ§indeki her satÄ±rÄ± dÃ¶ner,
    her satÄ±rdan get() ile istenen bilgileri alÄ±r(eÄŸer yoksa diye -> get("fdf", get()) kullanÄ±ldÄ±)
    her bir satÄ±rdaki veriler bir fonksiyon ile paketlenerek bir array'de tutuldu 
    """
    def process_diseases(self):
        self.processed_data = []

        for idx, row in self.df.iterrows():
            hastalik = str(row.get("hastalik", row.get("HastalÄ±k", ""))).strip()
            bolum = str(row.get("bolum", "")).strip()
            link = str(row.get("link", row.get("Link", ""))).strip()

            if not hastalik:
                continue


            # her chunk'da ortak olacak metadata
            base_metadata = {
                "hastalik": hastalik,
                "bolum": bolum,
                "link": link,
                "doc_type": "disease",
            }

            sections = [
                ("nedir",       row.get("nedir", row.get("Nedir", "")),       "NEDÄ°R"),
                ("belirtiler",  row.get("belirtiler", row.get("Belirtiler", "")), "BELÄ°RTÄ°LER"),
                ("turler",      row.get("tÃ¼rleri", row.get("TÃ¼rler", "")),   "TÃœRLER"),
                ("teshis",      row.get("teshis", row.get("TeÅŸhis", "")),    "TEÅHÄ°S"),
                ("tedavi",      row.get("tedavi", row.get("Tedavi", "")),    "TEDAVÄ°"),
                ("soru_cevap",  row.get("soru_cevap", row.get("Soru-Cevap", "")), "SORU-CEVAP"),
            ]

            # Her section iÃ§in farklÄ± bir chunk oluÅŸturma
            for section_key, content, title in sections:
                content = str(content).strip()
                
                # eÄŸer section boÅŸsa atla(nedir,belirtiler vs.)
                if not content:
                    continue

                #chunk metni
                text = f"""
                    HASTALIK: {hastalik}
                    BÃ–LÃœM: {bolum}
                    KISIM: {title}

                    {content}
                """.strip()

                # tam chunk yapÄ±sÄ±
                doc = {
                    "id": f"{hastalik.lower().replace(' ', '_')}__{section_key}",
                    "text": text,
                    "metadata": {
                        **base_metadata,
                        "section": section_key,
                        "section_title": title,
                    },
                }
                self.processed_data.append(doc)

        print(f"âœ… Toplam {len(self.processed_data)} RAG dokÃ¼manÄ± oluÅŸturuldu")
        return self.processed_data

    
    def _create_disease_document(self, **kwargs):
        hastalik_adi = kwargs.get("hastalik", "").strip()

        # Ana dÃ¶kÃ¼man metni - RAG'Ä±n arayacaÄŸÄ± text
        # Sorulacak sorulara cevap iÃ§in kullanÄ±lacak verileri burada ekle
        document_text = f"""
            HASTALIK: {hastalik_adi}

            NEDÄ°R: {kwargs.get('nedir', '')}

            BELÄ°RTÄ°LER: {kwargs.get('belirtiler', '')}

            TÃœRLER: {kwargs.get('tÃ¼rleri', '')}

            TEÅHÄ°S: {kwargs.get('teshis', '')}

            TEDAVÄ°: {kwargs.get('tedavi', '')}

            SORU-CEVAP: {kwargs.get('soru_cevap', '')}
        
        """.strip()

        # Metadata - Filtreleme iÃ§in kullanÄ±lacak
        metadata = {
            "hastalik": hastalik_adi,
            "bolum": kwargs.get("bolum", "").strip(),
            "link": kwargs.get("link", "").strip(),
            "doc_type": "disease",
        }

        return {
            "id": hastalik_adi.lower().replace(" ", "_"),
            "text": document_text,
            "metadata": metadata,
        }
   
    # eÄŸer bir section uzun ise alt parÃ§alara bÃ¶lme
    def chunk_large_documents(self, max_chunk_size=1000):
       
        """
        Uzun dÃ¶kÃ¼manlarÄ± parÃ§ala
        
        Ä°ÅŸlenen her bir section ÅŸeklindeki chunk , eÄŸer uzun bir text iÃ§eriyorsa
        cÃ¼mle bazlÄ± olarak parÃ§alara ayrÄ±lÄ±yor.      
        """
    
        print("âœ‚ï¸  Uzun dokÃ¼manlar parÃ§alanÄ±yor (section bazlÄ±)...")

        chunked_data = []

        for doc in self.processed_data:
            text = doc["text"]

            # KÃ¼Ã§Ã¼kse olduÄŸu gibi ekle
            if len(text) <= max_chunk_size:
                chunked_data.append(doc)
                continue
            
            lines = text.split("\n")
            header_lines = []
            content_start_idx = 0
            
            
            # HEADER + CONTENT OLARA PARÃ‡ALAMA(her section iÃ§in) 
            # Header satÄ±rlarÄ±nÄ± bul (HASTALIK:, BÃ–LÃœM:, KISIM: ile baÅŸlayanlar)
            # Bu header kÄ±smÄ± her parÃ§alÄ± seciton'Ä±n text kÄ±smÄ±nÄ±n baÅŸÄ±na eklenecek
            # bu sayede semantic olarak embedding sÄ±rasÄ±nda Ã§ok daha iyi eÅŸleÅŸme olacak
            for i, line in enumerate(lines):
                line_stripped = line.strip()
                if line_stripped.startswith(("HASTALIK:", "BÃ–LÃœM:", "KISIM:")):
                    header_lines.append(line)
                elif line_stripped and header_lines:
                    # Ä°lk iÃ§erik satÄ±rÄ±na geldi, header bitti
                    content_start_idx = i
                    break
            
            # Header'Ä± birleÅŸtir
            header = "\n".join(header_lines)
            if header:
                header += "\n\n"  #header sonu boÅŸluk
            
            # Ä°Ã§eriÄŸi al , header'dan sonraki text
            content = "\n".join(lines[content_start_idx:]).strip()
            

            # Ä°Ã§eriÄŸi cÃ¼mlelere bÃ¶l, uzun olanlarÄ± parÃ§ala(chunking)
            sentences = content.split(". ")
            current_chunk_content = ""
            chunk_index = 0

            for sent in sentences:
                if not sent.strip():
                    continue

                tentative = (current_chunk_content + " " + sent).strip()
                
                # Header boyutunu da hesaba katÄ±yor
                chunk_and_header = header + tentative
                
                if len(chunk_and_header) > max_chunk_size and current_chunk_content:
                    # Mevcut chunk'Ä± kaydet (HEADER + iÃ§erik)
                    chunked_data.append({
                        "id": f"{doc['id']}_chunk_{chunk_index}",
                        "text": (header + current_chunk_content).strip(),
                        "metadata": {
                            **doc["metadata"],
                            "chunk_index": chunk_index,
                            "is_sub_chunk": True,
                        },
                    })
                    chunk_index += 1
                    current_chunk_content = sent
                else:
                    current_chunk_content = tentative

            # Son chunk'Ä± ekle (HEADER + iÃ§erik)
            if current_chunk_content.strip():
                chunked_data.append({
                    "id": f"{doc['id']}_chunk_{chunk_index}",
                    "text": (header + current_chunk_content).strip(),
                    "metadata": {
                        **doc["metadata"],
                        "chunk_index": chunk_index,
                        "is_sub_chunk": True,
                    },
                })

        print(f"âœ… {len(self.processed_data)} dokÃ¼man â†’ {len(chunked_data)} chunk")
        self.processed_data = chunked_data
        return chunked_data
    
    def save_processed_data(self, output_path):
        """Ä°ÅŸlenmiÅŸ veriyi JSON olarak kaydet"""

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.processed_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Ä°ÅŸlenmiÅŸ veri kaydedildi: {output_file}")

    def get_statistics(self):
        """Veri istatistiklerini gÃ¶ster"""

        if not self.processed_data:
            print("âš ï¸  HenÃ¼z veri iÅŸlenmedi")
            return

        bolumler = {}
        for doc in self.processed_data:
            bolum = doc["metadata"].get("bolum", "Bilinmiyor")
            bolumler[bolum] = bolumler.get(bolum, 0) + 1

        print("\nğŸ“Š Veri Ä°statistikleri:")
        print(f"   - Toplam dÃ¶kÃ¼man: {len(self.processed_data)}")
        print(f"   - BÃ¶lÃ¼m sayÄ±sÄ±: {len(bolumler)}")
        print("\n   En Ã§ok hastalÄ±k olan bÃ¶lÃ¼mler:")

        sorted_bolumler = sorted(bolumler.items(), key=lambda x: x[1], reverse=True)
        for bolum, count in sorted_bolumler[:10]:
            print(f"     â€¢ {bolum}: {count} hastalÄ±k")


# Test iÃ§in 
if __name__ == "__main__":
    loader = DiseaseDataLoader("data/raw/hastaliklar_detayli_listesi.csv")

    if loader.load_csv():
        loader.process_diseases()
        loader.chunk_large_documents(max_chunk_size=1000)
        loader.save_processed_data("data/processed/diseases_processed.json")
        loader.get_statistics()
