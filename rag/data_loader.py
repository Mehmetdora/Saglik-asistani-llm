import pandas as pd
import json
from pathlib import Path
from typing import List, Dict

import os
import sys
import csv

class DiseaseDataLoader:

    def __init__(self, csv_path: str):
        print(f"Ana dosya yolu : {os.getcwd()}")
        current_path = os.getcwd()
        project_path = os.chdir('..')
        csv_path = os.path.abspath(csv_path)

        if not os.path.exists(csv_path):
            print(f"------>  HatalÄ± dosya yolu girilmiÅŸtir, girilen dosya yolu : 'f{csv_path}'")
        self.csv_path = Path(csv_path)
        self.df = None
        self.processed_data = []
        
        
        """
        
        HazÄ±rlanan CSV dosyasÄ± alÄ±nÄ±r , RAG sisteminin kullanabileceÄŸi ÅŸekilde
        dÃ¶kÃ¼man + metadata + chunk olarak parÃ§alar. 
        SonrasÄ±nda bu her bir chunk parÃ§asÄ±nÄ± bir json dosyasÄ±na kaydeder. 
        
        """

    def load_csv(self):
        """
        csv yi oku , 
        self.df e kaydet. 
        """
        print(f"ğŸ“‚ CSV yÃ¼kleniyor: {self.csv_path}")

        try:
            # CSV dosyasÄ±ndaki bazÄ± verilerin uzunluÄŸundan Ã¶tÃ¼rÃ¼ veri okumak iÃ§in max ayarlar seÃ§ildi
            csv.field_size_limit(sys.maxsize)

            # Pandas ile BOM-safe okuma, bazÄ± sÃ¼tun isimleri farklÄ± gelebiliyor
            self.df = pd.read_csv(
                self.csv_path,
                encoding='utf-8-sig',  # BOM'u otomatik kaldÄ±rÄ±r
                engine='python',
                quoting=csv.QUOTE_ALL
            )

            # Yine de sÃ¼tun isimlerini temizle
            self.df.columns = self.df.columns.str.strip()

            return True
        except Exception as e:
            print(f"âŒ Hata: {e}")
            return False
    
    """
    self.df iÃ§indeki her satÄ±rÄ± dÃ¶ner,
    her satÄ±rdan get() ile istenen bilgileri alÄ±r(eÄŸer yoksa diye -> get("fdf", get()) kullanÄ±ldÄ±)
    her bir satÄ±rdaki veriler bir fonksiyon ile paketlenerek bir array'de tutuldu 
    """
    def process_diseases(self):
        self.processed_data = []


        for idx, row in self.df.iterrows():
            hastalik = str(row.get("hastalik", row.get("HastalÄ±k", ""))).strip()
            bolum = str(row.get("bolum", "")).strip()
            link = str(row.get("link", row.get("Link", ""))).strip()

            if not hastalik:
                continue


            # her chunk'da ortak olacak metadata
            base_metadata = {
                "hastalik": hastalik,
                "bolum": bolum,
                "link": link,
                "doc_type": "disease",
            }

            sections = [
                ("nedir",       row.get("nedir", row.get("Nedir", "")),       "NEDÄ°R"),
                ("belirtiler",  row.get("belirtiler", row.get("Belirtiler", "")), "BELÄ°RTÄ°LER"),
                ("turler",      row.get("tÃ¼rleri", row.get("TÃ¼rler", "")),   "TÃœRLER"),
                ("teshis",      row.get("teshis", row.get("TeÅŸhis", "")),    "TEÅHÄ°S"),
                ("tedavi",      row.get("tedavi", row.get("Tedavi", "")),    "TEDAVÄ°"),
                ("soru_cevap",  row.get("soru_cevap", row.get("Soru-Cevap", "")), "SORU-CEVAP"),
            ]

            """
            EÄŸer veri iÃ§indeki bir hastalÄ±ÄŸÄ±n herhangi bir alt baÅŸlÄ±ÄŸÄ± null ise o baÅŸlÄ±ÄŸÄ± 
            bir chunk olarak oluÅŸturmamalÄ±.
            """


            # Her section iÃ§in farklÄ± bir chunk oluÅŸturma
            for section_key, content, title in sections:

                content = str(content).strip()
                
                # eÄŸer section boÅŸsa atla(nedir,belirtiler vs.), bu bilgiler de doldurulmalÄ± aslÄ±nda
                if pd.isnull(content) or content == '' or content == 'nan':
                    print(f"Sorunlu iÃ§erik : \n{content}")
                    continue

                # hastalÄ±k modeli
                data = {
                    "hastalik": hastalik,
                    "bolum": bolum,
                    "section": title,
                    "content": content, # ana iÃ§erik burada , sadece ilgili alt baÅŸlÄ±ÄŸÄ±n iÃ§eriÄŸi
                    "text_content": f"""HASTALIK: {hastalik}, BÃ–LÃœM: {bolum}, SECTÄ°ON: {title}, CONTENT: {content}""".strip(),
                }

                # tam chunk yapÄ±sÄ±
                doc = {
                    "id": f"{hastalik.lower().replace(' ', '_')}__{section_key}",
                    "data": data,
                    "metadata": {
                        **base_metadata,
                        "section": section_key,
                        "section_title": title,
                    },
                }

                self.processed_data.append(doc)

        print(f"---> Toplam {len(self.processed_data)} hastalÄ±k alt baÅŸlÄ±ÄŸÄ± oluÅŸturuldu")
        return self.processed_data


    # eÄŸer bir section uzun ise alt parÃ§alara bÃ¶lme
    # yani her hastalÄ±ÄŸÄ±n her bir alt baÅŸlÄ±ÄŸÄ±(nedir, belirtiler vs.) baÅŸlÄ±ÄŸÄ± kendi iÃ§inde chunklanÄ±yor
    """
    Sonda tek cÃ¼mle kalmamasÄ±nÄ± saÄŸlama: Ã‡oÄŸu normal durumda evet; son kÃ¼Ã§Ã¼k chunkâ€™Ä± bir Ã¶ncekiyle birleÅŸtiriyor.
    Chunkâ€™larÄ±n cÃ¼mle sayÄ±sÄ±nÄ±n aÅŸaÄŸÄ± yukarÄ± dengeli : Greedy + son merge ile cÃ¼mle sayÄ±larÄ± birbirine yakÄ±n chunklar oluÅŸturuldu, tam eÅŸit deÄŸil.
    """
    """
    Bu chunking ile cÃ¼mle sayÄ±sÄ±na gÃ¶re metin baÅŸtan sonra doÄŸru max karakter sayÄ±sÄ±nÄ± geÃ§meyecek ÅŸekilde cÃ¼mlelere gÃ¶re gruplanÄ±yor.
    Ama chunklar arasÄ±nda oluÅŸabilecek karater sayÄ±sÄ± farkÄ± kontrol edilmiyor, Ã¶zellikle sonuncu chunkda.
    """
    def chunk_large_documents(self, max_chunk_size=1000):
       
        """
        Uzun dÃ¶kÃ¼manlarÄ± parÃ§ala
        
        Ä°ÅŸlenen her bir section ÅŸeklindeki chunk , eÄŸer uzun bir text iÃ§eriyorsa
        cÃ¼mle bazlÄ± olarak parÃ§alara ayrÄ±lÄ±yor.      
        """
    
        print("---> Uzun hastalÄ±k alt baÅŸlÄ±klarÄ± parÃ§alanÄ±yor (section bazlÄ±)...")

        chunked_data = []

        for doc in self.processed_data:

            data = doc["data"]  # ana iÃ§erik

            # AsÄ±l content alanÄ±n hangisiyse onu kullan:
            # content = data["text_content"]
            content = data.get("content") or data.get("text_content") or ""

            header = f"HASTALIK: {data['hastalik']}, BÃ–LÃœM: {data['bolum']}, SECTÄ°ON: {data['section']}, CONTENT: "

            # KÃ¼Ã§Ã¼kse olduÄŸu gibi ekle
            if len(header) + len(content) <= max_chunk_size:
                chunked_data.append(
                    {
                        "id": f"{doc['id']}_chunk_0",
                        "text": header + content.strip(),
                        "metadata": {
                            **doc["metadata"],
                            "chunk_index": 0,
                            "is_sub_chunk": False,
                        },
                    }
                )
                continue


            raw_sentences = content.split(". ")
            sentences = [s.strip() for s in raw_sentences if s.strip()]


            temp_chunks = []
            current_chunk_sents = []

            for i, sent in enumerate(sentences):

                # Bu cÃ¼mleyi eklersek oluÅŸacak text (header + cÃ¼mleler)
                tentative_sents = current_chunk_sents + [sent]
                tentative_text = ". ".join(tentative_sents)
                tentative_full = header + tentative_text

                if len(tentative_full) > max_chunk_size and current_chunk_sents:
                    temp_chunks.append(current_chunk_sents)
                    current_chunk_sents = [sent]  # yeni chunk bu cÃ¼mle ile baÅŸlasÄ±n
                else:
                    current_chunk_sents.append(sent)

            # DÃ¶ngÃ¼ bitince elde kalan cÃ¼mleler
            if current_chunk_sents:
                temp_chunks.append(current_chunk_sents)

            # 3) En sondaki chunk Ã§ok kÃ¼Ã§Ã¼kse (Ã¶rneÄŸin tek cÃ¼mle) â†’ bir Ã¶ncekiyle birleÅŸtir
            if len(temp_chunks) >= 2:
                last_chunk = temp_chunks[-1]
                prev_chunk = temp_chunks[-2]

                last_text = ". ".join(last_chunk)
                prev_text = ". ".join(prev_chunk)

                # Karakter uzunluÄŸu ve cÃ¼mle sayÄ±sÄ±na gÃ¶re "Ã§ok kÃ¼Ã§Ã¼k" kontrolÃ¼
                # Ã–rneÄŸin: tek cÃ¼mle VE toplam uzunluÄŸun %30'undan az ise
                if (
                        len(last_chunk) < 2
                        or len(header) + len(last_text) < int(0.3 * max_chunk_size)
                ):
                    merged_text = prev_text + ". " + last_text
                    if len(header) + len(merged_text) <= max_chunk_size:
                        # BirleÅŸtirmeyi gÃ¼venle yapabiliyorsak:
                        temp_chunks[-2] = prev_chunk + last_chunk
                        temp_chunks.pop()  # son chunk'Ä± listeden Ã§Ä±kar

            # 4) ArtÄ±k temp_chunks iÃ§inde dÃ¼zgÃ¼n, dengeli gruplar var â†’ gerÃ§ek chunk objeleri Ã¼ret
            chunk_index = 0
            for sents in temp_chunks:
                chunk_text = ". ".join(sents).strip()
                full_text = header + chunk_text

                chunked_data.append({
                    "id": f"{doc['id']}_chunk_{chunk_index}",
                    "text": full_text,
                    "metadata": {
                        **doc["metadata"],
                        "chunk_index": chunk_index,
                        "is_sub_chunk": True,
                    },
                })
                chunk_index += 1

        print(f"--->  {len(self.processed_data)} dokÃ¼man â†’ {len(chunked_data)} chunk")
        self.processed_data = chunked_data
        return chunked_data

    """
    Metindeki cÃ¼mleler karakter sayÄ±larÄ±na gÃ¶re gruplanÄ±r. EÄŸer bir chunk iÃ§inde az karakter varsa ve komÅŸu chunklarÄ± ile 
    arasÄ±nda Ã§ok fark varsa onlardan cÃ¼mleler az olana kaydÄ±rÄ±larak olabildiÄŸince karakter farkÄ± azaltÄ±lÄ±r
    """
    def chunk_large_documents2(self, max_chunk_size=1000):
        """
        Uzun dÃ¶kÃ¼manlarÄ± parÃ§ala

        - komÅŸu chunklar arasÄ±nda cÃ¼mle kaydÄ±rarak boyutlarÄ± dengelenir
        """

        chunked_data = []

        # chunk uzunluÄŸunu karakter bazlÄ± hesapla
        def chunk_len(header: str, sents: list[str]) -> int:
            if not sents:
                return len(header)
            return len(header) + len(". ".join(sents))

        # YardÄ±mcÄ±: greedy chunk + balancing
        def split_and_balance(content: str, header: str, max_size: int) -> list[list[str]]:

            raw_sentences = content.split(". ")
            sentences = [s.strip() for s in raw_sentences if s.strip()]
            if not sentences:
                return []

            # 2) greedy chunking , limiti doldurana kadar baÅŸtan baÅŸlayarak cÃ¼mleler gruplanÄ±r
            temp_chunks: list[list[str]] = []
            current: list[str] = []

            for sent in sentences:
                tentative_sents = current + [sent]
                tentative_len = chunk_len(header, tentative_sents)

                if tentative_len > max_size and current:
                    temp_chunks.append(current)
                    current = [sent]
                else:
                    current = tentative_sents

            if current:
                temp_chunks.append(current)

            # dengeleme
            # amaÃ§: iki chunk arasÄ±ndaki karakter sayÄ±sÄ± Ã§ok fazla ise aradaki farkÄ± kapatmak iÃ§in az olana cÃ¼mle kaydÄ±rÄ±lÄ±r
            max_loops = 50  # max dÃ¶ngÃ¼ denemesi
            loop = 0

            def effective_len(sents: list[str]) -> int:
                return chunk_len(header, sents)

            while loop < max_loops:
                loop += 1
                changed = False

                # BoÅŸ chunk varsa kaldÄ±r
                temp_chunks = [c for c in temp_chunks if c]

                if len(temp_chunks) <= 1:
                    break

                for i in range(len(temp_chunks) - 1):
                    a = temp_chunks[i]
                    b = temp_chunks[i + 1]

                    len_a = effective_len(a)
                    len_b = effective_len(b)
                    diff = abs(len_a - len_b)

                    # Fark max karakter sayÄ±sÄ±nÄ±n %30 undan az ise yeterli
                    if diff < int(max_size * 0.3):
                        continue

                    # a bÃ¼yÃ¼k, b kÃ¼Ã§Ã¼k ise: a nÄ±n son cÃ¼mlesini b ye kaydÄ±rma
                    if len_a > len_b and len(a) > 1:
                        candidate = a[-1]
                        new_a = a[:-1]
                        new_b = [candidate] + b

                        if effective_len(new_a) <= max_size and effective_len(new_b) <= max_size:
                            temp_chunks[i] = new_a
                            temp_chunks[i + 1] = new_b
                            changed = True

                    # b bÃ¼yÃ¼k, a kÃ¼Ã§Ã¼k ise: b nin ilk cÃ¼mleyi a ya kaydÄ±rma
                    elif len_b > len_a and len(b) > 1:
                        candidate = b[0]
                        new_a = a + [candidate]
                        new_b = b[1:]

                        if effective_len(new_a) <= max_size and effective_len(new_b) <= max_size:
                            temp_chunks[i] = new_a
                            temp_chunks[i + 1] = new_b
                            changed = True

                if not changed:
                    break  # tÃ¼m chunklar kontrol edilmiÅŸse bitir

            # boÅŸ chunk kontrolÃ¼
            temp_chunks = [c for c in temp_chunks if c]
            return temp_chunks


        for doc in self.processed_data:
            data = doc["data"]  # ana metinler

            content = data.get("content") or data.get("text_content") or ""
            header = (
                f"HASTALIK: {data['hastalik']}, "
                f"BÃ–LÃœM: {data['bolum']}, "
                f"SECTÄ°ON: {data['section']}, CONTENT: "
            )

            # zaten geÃ§miyorsa direkt ekle
            if len(header) + len(content) <= max_chunk_size:
                chunked_data.append(
                    {
                        "id": f"{doc['id']}_chunk_0",
                        "text": header + content.strip(),
                        "metadata": {
                            **doc["metadata"],
                            "chunk_index": 0,
                            "is_sub_chunk": False,
                        },
                    }
                )
                continue

            # iÃ§eriÄŸi bÃ¶l ve dengele
            balanced_chunks_sents = split_and_balance(content, header, max_chunk_size)


            chunk_index = 0
            for sents in balanced_chunks_sents:
                chunk_text = ". ".join(sents).strip()
                full_text = header + chunk_text

                chunked_data.append(
                    {
                        "id": f"{doc['id']}_chunk_{chunk_index}",
                        "text": full_text,
                        "metadata": {
                            **doc["metadata"],
                            "chunk_index": chunk_index,
                            "is_sub_chunk": True,
                        },
                    }
                )
                chunk_index += 1

        print(f"--->  {len(self.processed_data)} dokÃ¼man â†’ {len(chunked_data)} chunk")
        self.processed_data = chunked_data
        return chunked_data

    def save_processed_data(self, output_path):
        """Ä°ÅŸlenmiÅŸ veriyi JSON olarak kaydet"""

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.processed_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Ä°ÅŸlenmiÅŸ veri kaydedildi: {output_file}")

    def get_statistics(self):
        """Veri istatistiklerini gÃ¶ster"""

        if not self.processed_data:
            print("---> HenÃ¼z veri iÅŸlenmedi")
            return

        bolumler = {}
        for doc in self.processed_data:
            bolum = doc["metadata"].get("bolum", "Bilinmiyor")
            bolumler[bolum] = bolumler.get(bolum, 0) + 1

        print("\n===> Veri Ä°statistikleri:")
        print(f"   - Toplam dÃ¶kÃ¼man: {len(self.processed_data)}")
        print(f"   - BÃ¶lÃ¼m sayÄ±sÄ±: {len(bolumler)}")
        print("\n   En Ã§ok chunk iÃ§eren bÃ¶lÃ¼mler:")

        sorted_bolumler = sorted(bolumler.items(), key=lambda x: x[1], reverse=True)
        for bolum, count in sorted_bolumler[:10]:
            print(f"     â€¢ {bolum}: {count} chunk")





# Test iÃ§in 
if __name__ == "__main__":
    loader = DiseaseDataLoader("data/raw/hastaliklar_detayli_listesi.csv")
    if loader.load_csv():
        loader.process_diseases()
        loader.chunk_large_documents2(max_chunk_size=1000)
        loader.save_processed_data("data/processed/diseases_processed.json")
        loader.get_statistics()
